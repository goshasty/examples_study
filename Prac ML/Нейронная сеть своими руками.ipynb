{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 практическое задание. Обучение полносвязной нейронной сети.\n",
    "\n",
    "## Практикум на ЭВМ для 317 группы, весна 2019\n",
    "\n",
    "#### Фамилия, имя: Демин Георгий\n",
    "\n",
    "Дата выдачи: 19 февраля\n",
    "\n",
    "Мягкий дедлайн: 28 февраля 23:59 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация нейронной сети (6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой нелинейности ReLU\n",
    "\n",
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        res = copy.deepcopy(X)\n",
    "        res[res <= 0] = 0\n",
    "        self.objs_mask = [res > 0]\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        \n",
    "        grad_input = np.zeros(grad_output.shape)\n",
    "        grad_input[tuple(self.objs_mask)] = 1 #!\n",
    "        grad_input *= grad_output        \n",
    "        return grad_input, []\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  1.],\n",
       "        [ 0., -0.]]), [])"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = ReLU()\n",
    "X = np.array([[1, 1], [-2, -2]])\n",
    "rl.forward(X)\n",
    "rl.backward(np.array([[1, 1], [2, -3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязный слой\n",
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale=(2/(input_units+output_units)), \n",
    "                                        size=(input_units, output_units) )\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        result = np.dot(X, self.weights)\n",
    "        result = result + self.biases[np.newaxis, :]\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        \n",
    "        grad_objs = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.X.T, grad_output)\n",
    "        grad_bias = np.dot(np.ones(shape=(1, grad_output.shape[0])), grad_output)\n",
    "        \n",
    "        return grad_objs, np.r_[grad_weights.ravel(), grad_bias.ravel()]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 2., 3.])"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Dense(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00334638, -0.11873199, -0.01232712,  0.04435959],\n",
       "        [ 0.22207476,  0.44739431, -0.09993307, -0.10118681],\n",
       "        [ 0.0096758 , -0.13386654, -0.06056873,  0.18862446]]),\n",
       " array([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка градиента\n",
    "\n",
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    \n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "### your code here\n",
    "rl = ReLU()\n",
    "\n",
    "rl.forward(points)\n",
    "output_grad = np.ones(points.shape)\n",
    "grads = rl.backward(output_grad)[0]\n",
    "\n",
    "def func_max(x):\n",
    "    res = copy.deepcopy(x) \n",
    "    res[res <= 0] = 0\n",
    "    return res.sum()\n",
    "    \n",
    "\n",
    "numeric_grads = eval_numerical_gradient(func_max, points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "el = Dense(12, 32)\n",
    "### your code here\n",
    "el.forward(x)\n",
    "grads = el.backward(np.ones(shape=(10, 32)))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: el.forward(x).sum(), x)\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "from scipy.special import softmax\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26894142, 0.73105858],\n",
       "       [0.01798621, 0.98201379]])"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 1], [-4 , 0]])\n",
    "softmax(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        self.input_X = X\n",
    "        res = X - logsumexp(X, axis=1)[:, np.newaxis]\n",
    "        return res\n",
    "        \n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        \n",
    "        res = softmax(self.input_X, axis=1) + grad_output\n",
    "        return res, []\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    res = activations*target*(-1)\n",
    "    res = (res[res!=0]).sum()\n",
    "    return res\n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    \n",
    "    return target * (-1)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "target = np.arange(10)\n",
    "target = np.eye(N=10, M=12)\n",
    "\n",
    "\n",
    "sm = Softmax()\n",
    "sm.forward(points)\n",
    "\n",
    "res = crossentropy(sm.forward(points), target)\n",
    "gr_out = grad_crossentropy(sm.forward(points), target)\n",
    "\n",
    "\n",
    "grads = sm.backward(gr_out)[0]\n",
    "\n",
    "def func(x):   \n",
    "    sm = Softmax()   \n",
    "    target_ = np.eye(N=10, M=12)\n",
    "    res = crossentropy(sm.forward(x), target_)\n",
    "    return res\n",
    "\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(func, points)\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка и обучение нейронной сети (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_net(network, X):\n",
    "    next_input = X\n",
    "    for layer in network:\n",
    "        next_input = layer.forward(next_input)\n",
    "    return next_input\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    return np.argmax(forward_net(network, X), axis=1)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(network, X_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].reshape(param.shape)\n",
    "            i += l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = np.zeros((y_train.shape[0], 10))\n",
    "y_train_oh[np.arange(y_train.shape[0]), y_train] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_grad_net(network, grad_loss):\n",
    "    network.reverse()\n",
    "    try:\n",
    "        next_grad = grad_loss\n",
    "        list_grad_params = list()\n",
    "        for layer in network:\n",
    "            grads = layer.backward(next_grad)\n",
    "            #print(len(grads[1]))\n",
    "            next_grad = grads[0]\n",
    "            list_grad_params.insert(0, grads[1])\n",
    "    finally:\n",
    "        network.reverse()\n",
    "    \n",
    "    #print(list_grad_params[1])\n",
    "    merged = list(itertools.chain.from_iterable(list_grad_params))\n",
    "    #print(len(merged))\n",
    "    return np.array(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    ### your code here\n",
    "    \n",
    "    set_weights(weights, network)\n",
    "    next_input = X\n",
    "    activations = forward_net(network, X)\n",
    "    gr_out = grad_crossentropy(activations, y)\n",
    "    \n",
    "    return crossentropy(activations, y), calc_grad_net(network, gr_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3099.8923650206525"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_grad(get_weights(network), [network, X_train, y_train_oh])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3466,)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weights(network).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = list()\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())\n",
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00575779,  0.02052346,  0.00553812, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train_oh], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00575779,  0.02052346,  0.00553812, ..., -0.01669729,\n",
       "       -0.12997059, -0.15433442])"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test. Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predict(network, X_train), y_train))\n",
    "print(accuracy_score(predict(network, X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print_=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print_ = print_\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        \n",
    "        tr_ac = accuracy_score(predict(self.network, self.X_train), self.y_train)\n",
    "        test_ac = accuracy_score(predict(self.network, self.X_test), self.y_test)\n",
    "        self.train_acc.append(tr_ac)\n",
    "        self.test_acc.append(test_ac)\n",
    "        if self.print_:\n",
    "            print(tr_ac, test_ac)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = list()\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())\n",
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10616184112843356 0.08444444444444445\n",
      "0.10616184112843356 0.08444444444444445\n",
      "0.10616184112843356 0.08444444444444445\n",
      "0.2650334075723831 0.24\n",
      "0.26206384558277657 0.24888888888888888\n",
      "0.26874536005939126 0.26\n",
      "0.3578322197475872 0.33111111111111113\n",
      "0.3637713437268003 0.32666666666666666\n",
      "0.4484038604305865 0.4088888888888889\n",
      "0.5716406829992576 0.5422222222222223\n",
      "0.6139569413511508 0.5533333333333333\n",
      "0.5694135115070527 0.54\n",
      "0.6302895322939867 0.5888888888888889\n",
      "0.6807720861172977 0.6466666666666666\n",
      "0.696362286562732 0.6844444444444444\n",
      "0.7253155159613957 0.7288888888888889\n",
      "0.7312546399406088 0.72\n",
      "0.7342242019302153 0.7422222222222222\n",
      "0.7564959168522642 0.7644444444444445\n",
      "0.7609502598366741 0.7644444444444445\n",
      "0.7921306607275427 0.7688888888888888\n",
      "0.8099480326651819 0.7911111111111111\n",
      "0.7988121752041574 0.7844444444444445\n",
      "0.8173719376391982 0.8088888888888889\n",
      "0.8195991091314031 0.8155555555555556\n",
      "0.8188567186340014 0.8177777777777778\n",
      "0.821826280623608 0.8222222222222222\n",
      "0.838158871566444 0.8422222222222222\n",
      "0.8470675575352635 0.8422222222222222\n",
      "0.8574610244988864 0.8444444444444444\n",
      "0.8596881959910914 0.8511111111111112\n",
      "0.8678544914625093 0.8622222222222222\n",
      "0.8789903489235338 0.8511111111111112\n",
      "0.8878990348923533 0.8688888888888889\n",
      "0.8938381588715665 0.8711111111111111\n",
      "0.8975501113585747 0.8777777777777778\n",
      "0.8990348923533779 0.8733333333333333\n",
      "0.9057164068299925 0.8822222222222222\n",
      "0.9146250927988122 0.8977777777777778\n",
      "0.9153674832962138 0.9\n",
      "0.9161098737936154 0.8888888888888888\n",
      "0.9168522642910171 0.8888888888888888\n",
      "0.9131403118040089 0.8911111111111111\n",
      "0.9250185597624351 0.8933333333333333\n",
      "0.9287305122494433 0.9044444444444445\n",
      "0.9294729027468448 0.9088888888888889\n",
      "0.9294729027468448 0.9133333333333333\n",
      "0.9376391982182628 0.9066666666666666\n",
      "0.9361544172234595 0.9088888888888889\n",
      "0.9406087602078693 0.9133333333333333\n",
      "0.9465478841870824 0.92\n",
      "0.9502598366740905 0.9222222222222223\n",
      "0.9517446176688938 0.9222222222222223\n",
      "0.9576837416481069 0.9266666666666666\n",
      "0.9606533036377134 0.9288888888888889\n",
      "0.9621380846325167 0.9377777777777778\n",
      "0.96362286562732 0.94\n",
      "0.9643652561247216 0.9377777777777778\n",
      "0.9651076466221232 0.94\n",
      "0.9658500371195249 0.9333333333333333\n",
      "0.9673348181143281 0.94\n",
      "0.9673348181143281 0.9377777777777778\n",
      "0.9695619896065331 0.9444444444444444\n",
      "0.9703043801039347 0.9422222222222222\n",
      "0.9725315515961396 0.9444444444444444\n",
      "0.9755011135857461 0.9422222222222222\n",
      "0.9762435040831478 0.9466666666666667\n",
      "0.9762435040831478 0.9466666666666667\n",
      "0.9747587230883444 0.94\n",
      "0.9769858945805494 0.9466666666666667\n",
      "0.9806978470675576 0.9422222222222222\n",
      "0.9799554565701559 0.9422222222222222\n",
      "0.9799554565701559 0.9488888888888889\n",
      "0.9806978470675576 0.9466666666666667\n",
      "0.9806978470675576 0.9422222222222222\n",
      "0.9799554565701559 0.9466666666666667\n",
      "0.9836674090571641 0.9444444444444444\n",
      "0.9829250185597624 0.9422222222222222\n",
      "0.9844097995545658 0.9444444444444444\n",
      "0.9866369710467706 0.9488888888888889\n",
      "0.9903489235337788 0.9511111111111111\n",
      "0.9903489235337788 0.9466666666666667\n",
      "0.9903489235337788 0.9488888888888889\n",
      "0.9903489235337788 0.9488888888888889\n",
      "0.9910913140311804 0.9488888888888889\n",
      "0.9910913140311804 0.9466666666666667\n",
      "0.9896065330363771 0.9466666666666667\n",
      "0.9910913140311804 0.9444444444444444\n",
      "0.9910913140311804 0.9466666666666667\n",
      "0.9933184855233853 0.9444444444444444\n",
      "0.994060876020787 0.9444444444444444\n",
      "0.9948032665181886 0.9444444444444444\n",
      "0.9948032665181886 0.9488888888888889\n",
      "0.9985152190051967 0.9511111111111111\n",
      "0.9977728285077951 0.9488888888888889\n",
      "0.9977728285077951 0.9488888888888889\n",
      "0.9985152190051967 0.9488888888888889\n",
      "0.9985152190051967 0.9533333333333334\n",
      "0.9977728285077951 0.9555555555555556\n",
      "0.9977728285077951 0.96\n",
      "0.9977728285077951 0.9577777777777777\n",
      "0.9992576095025983 0.96\n",
      "0.9970304380103935 0.9533333333333334\n",
      "0.9992576095025983 0.9488888888888889\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9488888888888889\n",
      "0.9992576095025983 0.9488888888888889\n",
      "1.0 0.9466666666666667\n",
      "1.0 0.9488888888888889\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9555555555555556\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9555555555555556\n",
      "1.0 0.9555555555555556\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9488888888888889\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9555555555555556\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9555555555555556\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9488888888888889\n",
      "1.0 0.9555555555555556\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9488888888888889\n",
      "1.0 0.9511111111111111\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9533333333333334\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.96\n",
      "1.0 0.96\n",
      "1.0 0.96\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.9577777777777777\n",
      "1.0 0.96\n",
      "1.0 0.96\n",
      "1.0 0.96\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9644444444444444\n",
      "1.0 0.9644444444444444\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "1.0 0.9622222222222222\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print_=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train_oh], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)\n",
    "print(res['success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28e6dad1898>"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с числом слоев (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_layers = 7\n",
    "max_num_iters = 5\n",
    "accs_train = np.zeros((max_num_layers, max_num_iters))\n",
    "accs_test = np.zeros((max_num_layers, max_num_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "57\n",
      "58\n",
      "64\n",
      "61\n",
      "1 0\n",
      "82\n",
      "1 1\n",
      "77\n",
      "1 2\n",
      "89\n",
      "1 3\n",
      "61\n",
      "1 4\n",
      "100\n",
      "2 0\n",
      "157\n",
      "2 1\n",
      "110\n",
      "2 2\n",
      "140\n",
      "2 3\n",
      "177\n",
      "2 4\n",
      "135\n",
      "3 0\n",
      "183\n",
      "3 1\n",
      "276\n",
      "3 2\n",
      "335\n",
      "3 3\n",
      "160\n",
      "3 4\n",
      "260\n",
      "4 0\n",
      "351\n",
      "4 1\n",
      "244\n",
      "4 2\n",
      "282\n",
      "4 3\n",
      "221\n",
      "4 4\n",
      "260\n",
      "5 0\n",
      "393\n",
      "5 1\n",
      "712\n",
      "5 2\n",
      "607\n",
      "5 3\n",
      "435\n",
      "5 4\n",
      "373\n",
      "6 0\n",
      "546\n",
      "6 1\n",
      "466\n",
      "6 2\n",
      "476\n",
      "6 3\n",
      "602\n",
      "6 4\n",
      "1596\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_size = 32\n",
    "               \n",
    "for num_iter in range(0, max_num_iters):\n",
    "    network = list()\n",
    "    network.append(Dense(X_train.shape[1], 10))\n",
    "    network.append(Softmax())\n",
    "    weights = get_weights(network)\n",
    "    \n",
    "    res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train_oh], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True)\n",
    "    \n",
    "    tr_ac = accuracy_score(predict(network, X_train), y_train)\n",
    "    test_ac = accuracy_score(predict(network, X_test), y_test)\n",
    "    \n",
    "    accs_train[0, num_iter] = tr_ac\n",
    "    accs_test[0, num_iter] = test_ac\n",
    "    print(res['nit'])\n",
    "    \n",
    "\n",
    "for num_layers in range(1, max_num_layers):    \n",
    "    for num_iter in range(0, max_num_iters):               \n",
    "        print(num_layers, num_iter)\n",
    "        \n",
    "        network = list()\n",
    "        network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "        for i in range(1, num_layers):           \n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "        \n",
    "        network.append(ReLU())\n",
    "        network.append(Dense(hidden_layers_size, 10))\n",
    "        network.append(Softmax())\n",
    "        \n",
    "        weights = get_weights(network)\n",
    "        \n",
    "        res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train_oh], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True)\n",
    "    \n",
    "        tr_ac = accuracy_score(predict(network, X_train), y_train)\n",
    "        test_ac = accuracy_score(predict(network, X_test), y_test)\n",
    "    \n",
    "        accs_train[num_layers, num_iter] = tr_ac\n",
    "        accs_test[num_layers, num_iter] = test_ac\n",
    "    \n",
    "        print(res[\"nit\"])\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3100.5941996202146,\n",
       " array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         2.49184118e-05, -1.26071643e-04,  1.06704083e-04]))"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_grad(weights, [network, X_train, y_train_oh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = get_weights(network)\n",
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train_oh], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True)\n",
    "res['nit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94444444, 0.94222222, 0.94888889, 0.93777778, 0.94222222],\n",
       "       [0.95777778, 0.94444444, 0.95555556, 0.95777778, 0.94666667],\n",
       "       [0.94666667, 0.95555556, 0.94888889, 0.94888889, 0.94888889],\n",
       "       [0.94888889, 0.94      , 0.96      , 0.94222222, 0.93777778],\n",
       "       [0.94888889, 0.94666667, 0.93555556, 0.94888889, 0.92666667],\n",
       "       [0.94888889, 0.92222222, 0.92444444, 0.92888889, 0.92666667],\n",
       "       [0.92888889, 0.93333333, 0.93555556, 0.9       , 0.90444444]])"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test quality in 5 runs')"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим лучшее качество достигается при 2-3 слоях нейронной сети. При этом отличие от линейной модели не очень большое. Гораздо хуже качество получается при увеличении числа слоев (соответсвенно, усложнении модели), связано это, скорее всего, с переобучением сети (интересно отметить, что при всех эспериментах модель достигала 1 на обучающей выборке).\n",
    "\n",
    "\n",
    "\n",
    "Также отметим, что при большом числе слоев количество итераций, необходимых для оптимизации, растет. Связано это с проблемой просачивания градиента: чем больше слоев, тем хуже доходит градиент backpropogation до первых слоев и тем хуже обучаются веса на первых слоях. От этого может старадть устойчивость обучения (при различной инициализации сеть обучается лучше или хуже), однако на графике в нашем случае это не так очевидно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация метода оптимизации (1 балл)\n",
    "\n",
    "Реализуйте сами метод оптимизации (аналог функции minimize) для рассмотренной выше архитектуры. В качестве метода оптимизации используйте SGD + momentum. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с LBFGS-B. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout (1 балл) \n",
    "\n",
    "Реализуйте слой Dropout. Сравните обучение сети из большого числа слоёв при использовании Dropout и без его использования (предварительно подберите адекватный параметр p). Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNormalization (1 балл)\n",
    "\n",
    "Реализуйте слой BatchNormalization. Сравните обучение сети из большого числа слоёв при использовании BatchNormalization и без его использования.  Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
